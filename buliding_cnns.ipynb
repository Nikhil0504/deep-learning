{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3332d6ee",
   "metadata": {},
   "source": [
    "# Building Deep Learning Models with PyTorch\n",
    "\n",
    "**Objective**: By the end of this workshop, participants will have a basic understanding of how to build a deep learning model from scratch using PyTorch and will be able to apply this knowledge to their own projects.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Nikhil0504/deep-learning/blob/main/buliding_cnns.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "> NOTE: If you are using Google Colab, make sure to use this procedure to equip the GPU.\n",
    "**Runtime -> Change runtime type -> Hardware Accelerator (T4 GPU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24c5c3",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "Deep learning is a subfield of machine learning that involves the use of neural networks to analyze and interpret data. Neural networks are composed of multiple layers of interconnected nodes or \"neurons,\" which process and transform the input data.\n",
    "\n",
    "![image.png](https://www.ait.de/wp-content/uploads/2022/01/Vergleich-Machine-Learning-Deep-Learning-2048x1536.png)\n",
    "\n",
    "## Key Components of Deep Learning\n",
    "1. **Artifical Neurons (Nodes)**: These are the basic building blocks of a neural network. Each node receives one or more inputs, performs a computation on those inputs, and then sends the output to other nodes.\n",
    "2. **Connections (Edges)**: These are the links between nodes, which allow them to exchange information.\n",
    "3. **Activation Functions**: These are the mathematical functions that are applied to the output of each node to introduce non-linearity into the model.\n",
    "4. **Layers**: A neural network is composed of multiple layers, each containing a set of nodes. The first layer is the input layer, followed by one or more hidden layers, and finally the output layer.\n",
    "\n",
    "## What is PyTorch?\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research Lab (FAIR). It provides a dynamic computation graph and is particularly well-suited for rapid prototyping and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab6552",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized the field of computer vision. Unlike traditional neural networks, CNNs are specifically designed to process data with a grid-like topology, such as images.\n",
    "\n",
    "In this workshop, we'll build a CNN from scratch to understand its fundamental components, and then leverage the power of transfer learning with pre-trained models to see how modern deep learning approaches work in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40423ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from torchvision) (2.2.0)\n",
      "Collecting torch==2.6.0 (from torchvision)\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cpu\n",
      "    Uninstalling torch-2.5.1+cpu:\n",
      "      Successfully uninstalled torch-2.5.1+cpu\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d3dd06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms, models\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# Part 1: Setting up environment\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99739fe",
   "metadata": {},
   "source": [
    "## Dataset - Fashion MNIST\n",
    "Fashion MNIST was created by Zalando Research as a drop-in replacement for the original MNIST dataset. While MNIST contains handwritten digits (0-9), Fashion MNIST contains 70,000 grayscale images of clothing items across 10 categories.\n",
    "\n",
    "In this section, we will explore the dataset in detail, including how to load it, visualize some examples, and preprocess the images for training.\n",
    "\n",
    "Dataset Details:\n",
    "- Images: 28×28 pixel grayscale\n",
    "- Classes: 10 categories of clothing items\n",
    "- Training set: 60,000 images\n",
    "- Test set: 10,000 images\n",
    "- Format: Each image is normalized and centered\n",
    "\n",
    "The 10 categories are:\n",
    "| Class ID | Class Name       |\n",
    "|----------|------------------|\n",
    "| 0        | T-shirt/top      |\n",
    "| 1        | Trouser          |\n",
    "| 2        | Pullover         |\n",
    "| 3        | Dress            |\n",
    "| 4        | Coat             |\n",
    "| 5        | Sandal           |\n",
    "| 6        | Shirt            |\n",
    "| 7        | Sneaker          |\n",
    "| 8        | Bag              |\n",
    "| 9        | Ankle boot       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Data Preparation\n",
    "\n",
    "# Define transformations for the training and validation sets\n",
    "# The transform makes sure that the images are in the right format for the model\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89169875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training and testing\n",
    "# Batch size refers to the number of samples processed before the model is updated\n",
    "batch_size = 64 \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# TODO: Create DataLoader for test dataset (same as train_loader but with test_dataset)\n",
    "test_loader =  DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names for Fashion MNIST\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset\n",
    "def visualize_dataset(loader):\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Create a grid of images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(images[i].squeeze().numpy(), cmap='gray')\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8dc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to visualize the dataset\n",
    "visualize_dataset(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d4cbb",
   "metadata": {},
   "source": [
    "# Fundementals of CNNs and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7ee56e",
   "metadata": {},
   "source": [
    "## Understanding CNNs: The Building Blocks\n",
    "\n",
    "Before we build our model, let's understand the key components of a CNN:\n",
    "\n",
    "### 1. Convolutional Layers\n",
    "\n",
    "Convolutional layers are the core building blocks of CNNs. They work by:\n",
    "- Sliding a small filter (kernel) across the input image\n",
    "- Performing element-wise multiplication at each position\n",
    "- Summing the results to produce a feature map\n",
    "\n",
    "#### Key Parameters:\n",
    "- **in_channels**: Number of input channels (1 for grayscale, 3 for RGB)\n",
    "- **out_channels**: Number of filters/kernels to learn\n",
    "- **kernel_size**: Size of the convolutional filter (e.g., 3×3, 5×5)\n",
    "- **stride**: Step size when sliding the filter\n",
    "- **padding**: Adding zeros around the input to preserve spatial dimensions\n",
    "\n",
    "### 2. Activation Functions (ReLU)\n",
    "\n",
    "After convolution, we apply a non-linear activation function:\n",
    "- Introduces non-linearity to the model\n",
    "- Enables learning of complex patterns\n",
    "- ReLU (Rectified Linear Unit) is most common: f(x) = max(0, x)\n",
    "\n",
    "### 3. Pooling Layers\n",
    "\n",
    "Pooling reduces the spatial dimensions of the feature maps:\n",
    "- **Max Pooling**: Takes maximum value in each window\n",
    "- **Average Pooling**: Takes average value in each window\n",
    "- Reduces computational complexity\n",
    "- Provides translation invariance\n",
    "\n",
    "### 4. Fully Connected Layers\n",
    "\n",
    "After extracting features with convolutional layers:\n",
    "- Flatten the feature maps into a 1D vector\n",
    "- Connect to fully connected (dense) layers\n",
    "- Final layer outputs class probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a959fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_convolution_and_pooling():\n",
    "    # Get a single image\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    single_image = images[0].unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Define a simple filter for demonstration (edge detection)\n",
    "    edge_filter = torch.tensor([[-1, -1, -1],\n",
    "                                [-1,  8, -1],\n",
    "                                [-1, -1, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply convolution manually\n",
    "    conv_layer = nn.Conv2d(1, 1, kernel_size=3, bias=False)\n",
    "    conv_layer.weight.data = edge_filter\n",
    "    \n",
    "    # Get the feature map\n",
    "    with torch.no_grad():\n",
    "        feature_map = conv_layer(single_image)\n",
    "    \n",
    "    # Apply ReLU\n",
    "    relu_feature_map = F.relu(feature_map)\n",
    "    \n",
    "    # Apply max pooling\n",
    "    pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    pooled_feature_map = pool_layer(relu_feature_map)\n",
    "    \n",
    "    # Plot original image, feature map, and pooled feature map\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(single_image.squeeze().numpy(), cmap='gray')\n",
    "    plt.title(f'Original: {class_names[labels[0]]}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(feature_map.squeeze().numpy(), cmap='viridis')\n",
    "    plt.title('After Convolution')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(relu_feature_map.squeeze().numpy(), cmap='viridis')\n",
    "    plt.title('After ReLU')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(pooled_feature_map.squeeze().numpy(), cmap='viridis')\n",
    "    plt.title('After Max Pooling')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Visualizing Convolution, ReLU and Pooling Operations', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how convolution and pooling work\n",
    "visualize_convolution_and_pooling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fbf11",
   "metadata": {},
   "source": [
    "# Implementing a CNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: 1x28x28, Output: 32x26x26\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        # After ReLU: 32x26x26\n",
    "        \n",
    "        # First max pooling layer\n",
    "        # Input: 32x26x26, Output: 32x13x13\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # TODO: Second convolutional layer\n",
    "        # Input: 32x13x13, Output: 64x11x11\n",
    "        self.conv2 = ...\n",
    "        # After ReLU: 64x11x11\n",
    "        \n",
    "        # TODO: Second max pooling layer\n",
    "        # Input: 64x11x11, Output: 64x5x5\n",
    "        self.pool2 = ...\n",
    "        \n",
    "        # TODO: Fully connected layers\n",
    "        # Input: 64*5*5 = 1600\n",
    "        self.fc1 = ...\n",
    "\n",
    "        # TODO: Output: 128 (hidden layer)\n",
    "        self.fc2 = ...  # 10 classes for Fashion MNIST\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first conv layer, then ReLU activation, then pooling\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # TODO: Apply second conv layer, then ReLU activation, then pooling\n",
    "        x = ...\n",
    "        \n",
    "        # Flatten the output from the convolutional layers\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        \n",
    "        # TODO: Apply first fully connected layer with ReLU\n",
    "        x = ...\n",
    "        \n",
    "        # TODO: Apply final fully connected layer \n",
    "        # (no activation - will be applied in loss)\n",
    "        x = ...\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our model and move it to the device\n",
    "model_scratch = SimpleCNN().to(device)\n",
    "print(model_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Training the Model\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=5):\n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # Print statistics\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, '\n",
    "              f'Time: {elapsed_time:.2f}s')\n",
    "    \n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # No gradient calculation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "# CrossEntropyLoss is suitable for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "optimizer = optim.Adam(model_scratch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbcee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# TODO: fill the required parameters and call the train_model function\n",
    "train_losses, train_accuracies, test_losses, test_accuracies = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e55fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training progress\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5), dpi=150)\n",
    "\n",
    "ax[0].plot(train_losses, label='Train Loss')\n",
    "ax[0].plot(test_losses, label='Test Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Loss vs. Epoch')\n",
    "\n",
    "ax[1].plot(train_accuracies, label='Train Accuracy')\n",
    "ax[1].plot(test_accuracies, label='Test Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Accuracy vs. Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802abadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Visualise the CNN's results\n",
    "\n",
    "def visualize_predictions(model, loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Move to the same device as the model\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Move tensors back to CPU for plotting\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predicted = predicted.cpu()\n",
    "    \n",
    "    # Create a grid of images with predictions\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(images[i].squeeze().numpy(), cmap='gray')\n",
    "        color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "        plt.title(f'True: {class_names[labels[i]]}\\nPred: {class_names[predicted[i]]}', \n",
    "                 color=color)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions from our scratch model\n",
    "visualize_predictions(model_scratch, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9139a",
   "metadata": {},
   "source": [
    "## Introduction to Transfer Learning\n",
    "\n",
    "Transfer learning is a powerful technique in deep learning where a model developed for one task is reused as the starting point for a model on a second task. It's particularly effective when:\n",
    "\n",
    "1. You have limited training data\n",
    "2. The pre-trained model was trained on a related domain\n",
    "3. You want to reduce training time and computational resources\n",
    "\n",
    "### Why Transfer Learning Works:\n",
    "\n",
    "- Lower-level features (edges, textures, patterns) are often universal\n",
    "- Pre-trained models have already learned these features from millions of images\n",
    "- We only need to fine-tune the model to our specific task\n",
    "\n",
    "### The Process:\n",
    "\n",
    "1. **Take a pre-trained model** (like ResNet, VGG, etc.) trained on a large dataset (e.g., ImageNet)\n",
    "2. **Remove the final classification layer** (specific to the original task)\n",
    "3. **Add a new classification layer** for our number of classes\n",
    "4. **Freeze early layers** to preserve learned features\n",
    "5. **Train only the new layers** (or fine-tune some of the later layers)\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Faster convergence**: The model already understands basic visual features\n",
    "- **Better performance**: Especially with limited data\n",
    "- **Less computational resources**: Most of the network is already trained\n",
    "\n",
    "Let's implement transfer learning using ResNet-18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96831c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to adapt our approach for pre-trained models since Fashion MNIST is grayscale\n",
    "# and most pre-trained models expect 3-channel RGB images\n",
    "\n",
    "# Define a new transformation for using pre-trained models\n",
    "transform_pretrained = transforms.Compose([\n",
    "    # ImageNet stats see https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights\n",
    "    transforms.Grayscale(3),  # Convert grayscale to 3 channels\n",
    "    transforms.Resize(224),  # Resize to match pre-trained model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "# TODO: Load the Fashion MNIST dataset with the new transformations \n",
    "train_dataset_pretrained = ...\n",
    "test_dataset_pretrained = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create data loaders\n",
    "# Smaller batch size due to larger images\n",
    "batch_size_pretrained = 32  \n",
    "train_loader_pretrained = ...\n",
    "test_loader_pretrained = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad574f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet-18 model\n",
    "pretrained_model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model\n",
    "# as we can see, the model expects 3-channel input \n",
    "# and has 1000 output classes (for ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebf47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model for our task\n",
    "num_ftrs = pretrained_model.fc.in_features  # Get the number of input features for the last layer\n",
    "pretrained_model.fc = nn.Linear(num_ftrs, 10)  # Change output to 10 classes for Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c94a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the device\n",
    "pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eca75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze most of the network, only train the final layers\n",
    "for param in list(pretrained_model.parameters())[:-2]:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer for the pre-trained model\n",
    "criterion_pretrained = nn.CrossEntropyLoss()\n",
    "optimizer_pretrained = optim.Adam(filter(lambda p: p.requires_grad, pretrained_model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be35ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the pre-trained model\n",
    "num_epochs_pretrained = 3\n",
    "\n",
    "train_losses_pretrained, train_accuracies_pretrained, test_losses_pretrained, test_accuracies_pretrained = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training progress for the pre-trained model\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_pretrained, label='Train Loss')\n",
    "plt.plot(test_losses_pretrained, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Pre-trained Model: Loss vs. Epoch')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies_pretrained, label='Train Accuracy')\n",
    "plt.plot(test_accuracies_pretrained, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Pre-trained Model: Accuracy vs. Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models():\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Scratch CNN Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "    print(f\"Pre-trained Model Final Test Accuracy: {test_accuracies_pretrained[-1]:.2f}%\")\n",
    "    \n",
    "    improvement = test_accuracies_pretrained[-1] - test_accuracies[-1]\n",
    "    print(f\"\\nImprovement with Transfer Learning: {improvement:.2f}%\")\n",
    "    \n",
    "    # Visualize the comparison\n",
    "    models_names = ['Scratch CNN', 'Pre-trained ResNet-18']\n",
    "    final_accuracies = [test_accuracies[-1], test_accuracies_pretrained[-1]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models_names, final_accuracies, color=['skyblue', 'orange'])\n",
    "    plt.ylabel('Test Accuracy (%)')\n",
    "    plt.title('Final Test Accuracy Comparison')\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a476c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745cab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
